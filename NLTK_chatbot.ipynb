{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1EJ3+RMAarLmEGFW8xo+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vanshikaa1444/NLTK_chatbot/blob/main/NLTK_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KckiiJsrFMZq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f= open('/content/data.txt','r',errors='ignore')\n",
        "raw_doc= f.read()"
      ],
      "metadata": {
        "id": "gLZWBUQzFlLS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_doc = raw_doc.lower() #Converting entire text to lowercase\n",
        "nltk.download('punkt') #Using the Punkt tokenizer\n",
        "nltk.download('wordnet') #Using the wordnet dictionary\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RXgEl5XF8PR",
        "outputId": "65705586-3a08-4b9b-bbad-acf800725159"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
        "\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "id": "PHVkeKSGGwno"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After Tokenization\n"
      ],
      "metadata": {
        "id": "UPpcZlR0G3dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are printing 5 sentences"
      ],
      "metadata": {
        "id": "VuoQFmSyHEvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWkj4aiUG7xf",
        "outputId": "749445ab-2705-428a-9164-cf7090827d4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nmain menu\\n\\nwikipediathe free encyclopedia\\nsearch wikipedia\\nsearch\\ncreate account\\nlog in\\n\\npersonal tools\\ncontents hide\\n(top)\\nbackground\\ndevelopment\\napplication\\ntoggle application subsection\\nmessaging apps\\nas part of company apps and websites\\nchatbot sequences\\ncompany internal platforms\\ncustomer service\\nhealthcare\\npolitics\\ntoys\\nmalicious use\\nlimitations of chatbots\\nchatbots and jobs\\nsee also\\nreferences\\nfurther reading\\nexternal links\\nchatbot\\n\\narticle\\ntalk\\nread\\nview source\\nview history\\n\\ntools\\npage semi-protected\\nfrom wikipedia, the free encyclopedia\\nfor other uses, see chatbot (disambiguation).',\n",
              " 'parts of this article (those related to everything, particularly sections after the intro) need to be updated.',\n",
              " 'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).',\n",
              " 'please help update this article to reflect recent events or newly available information.',\n",
              " '(february 2023)\\n\\na virtual assistant chatbot\\n\\nthe 1966 eliza chatbot\\npart of a series on\\nmachine learning\\nand data mining\\nparadigms\\nproblems\\nsupervised learning\\n(classification â€¢ regression)\\nclustering\\ndimensionality reduction\\nstructured prediction\\nanomaly detection\\nartificial neural network\\nautoencodercognitive computingdeep learningdeepdreamfeedforward neural networkrecurrent neural network lstmgruesnreservoir computingrestricted boltzmann machinegandiffusion modelsomconvolutional neural network u-nettransformer visionmambaspiking neural networkmemtransistorelectrochemical ram (ecram)\\nreinforcement learning\\nlearning with humans\\nmodel diagnostics\\nmathematical foundations\\nmachine-learning venues\\nrelated articles\\nvte\\na chatbot (originally chatterbot[1]) is a software application or web interface that is designed to mimic human conversation through text or voice interactions.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YlTSDZ2HBlj",
        "outputId": "3c2e981d-5c0b-4b41-dedf-27d0f816fbaf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['main', 'menu', 'wikipediathe', 'free', 'encyclopedia']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing text-preprocessing steps\n"
      ],
      "metadata": {
        "id": "28GipjZoHiYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens (tokens):\n",
        "\n",
        " return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "\n",
        " return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
      ],
      "metadata": {
        "id": "5qKqPR5THTOW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining greeting functions"
      ],
      "metadata": {
        "id": "QxLXWe55IFhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "greet_inputs = ('hello', 'hi','whassup', 'how are you?')\n",
        "greet_responses = ('hi', 'Hey', 'Hey There!', 'There there!!')\n",
        "\n",
        "def greet(sentence):\n",
        "\n",
        " for word in sentence.split():\n",
        "\n",
        "  if word.lower() in greet_inputs:\n",
        "\n",
        "   return random.choice(greet_responses)"
      ],
      "metadata": {
        "id": "SAjEEM0RIEf8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation by the bot"
      ],
      "metadata": {
        "id": "0KPnWN3hIZBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "AsY4wdjYIdKJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "\n",
        " robo1_response = ''\n",
        "\n",
        " TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
        "\n",
        " tfidf= TfidfVec.fit_transform(sentence_tokens)\n",
        "\n",
        " vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "\n",
        " idx = vals.argsort()[0][-2]\n",
        "\n",
        " flat= vals.flatten()\n",
        "\n",
        " flat.sort()\n",
        "\n",
        " req_tfidf = flat[-2]\n",
        "\n",
        " if (req_tfidf == 0):\n",
        "\n",
        "  robo1_response = robo1_response + \"I am sorry. Unable to understand you!\"\n",
        "\n",
        "  return robo1_response\n",
        "\n",
        " else:\n",
        "\n",
        "  robo1_response = robo1_response+ sentence_tokens[idx]\n",
        "\n",
        "  return robo1_response"
      ],
      "metadata": {
        "id": "65VxnY7zJMEi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Degfining the chart flow"
      ],
      "metadata": {
        "id": "v-nvQgalJmru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag=True\n",
        "\n",
        "print(\"Hello! I am the Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye!\")\n",
        "\n",
        "while(flag== True):\n",
        "\n",
        " user_response = input()\n",
        "\n",
        " user_response =user_response.lower()\n",
        "\n",
        " if(user_response != 'bye'):\n",
        "\n",
        "  if(user_response == 'thank you' or user_response == 'thanks'):\n",
        "\n",
        "   flag = False\n",
        "\n",
        "   print('Bot: You are welcome..')\n",
        "\n",
        "  else:\n",
        "\n",
        "   if(greet(user_response) != None):\n",
        "\n",
        "     print('Bot:'+ greet(user_response))\n",
        "\n",
        "   else:\n",
        "\n",
        "    sentence_tokens.append(user_response)\n",
        "\n",
        "    word_tokens = word_tokens+ nltk.word_tokenize(user_response)\n",
        "\n",
        "    final_words=list(set(word_tokens))\n",
        "    print('Bot:', end = '')\n",
        "\n",
        "    print(response(user_response))\n",
        "    sentence_tokens.remove(user_response)\n",
        "\n",
        " else:\n",
        "\n",
        "  flag=False\n",
        "\n",
        "  print('Bot: Goodbye!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkq20Zk9Jln2",
        "outputId": "05113781-bf8e-4a88-933e-090af89925ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I am the Learning Bot. Start typing your text after greeting to talk to me. For ending convo type bye!\n",
            "hi\n",
            "Bot:Hey There!\n",
            "can you tell me about turing test\n",
            "Bot:chatbot competitions focus on the turing test or more specific goals.\n",
            "Alan Turing\n",
            "Bot:[8]\n",
            "\n",
            "background\n",
            "in 1950, alan turing's famous article \"computing machinery and intelligence\" was published,[10] which proposed what is now called the turing test as a criterion of intelligence.\n",
            "Thank you, bye\n",
            "Bot:I am sorry. Unable to understand you!\n",
            "bye\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    }
  ]
}